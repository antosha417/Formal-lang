{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ÐžÐ±ÑƒÑ‡ÐµÐ½Ð¸Ðµ Ñ Ð¿Ð¾Ð´ÐºÑ€ÐµÐ¿Ð»ÐµÐ½Ð¸ÐµÐ¼ (reinforcement learning)\n",
    "\n",
    "ÐžÐ±ÑƒÑ‡ÐµÐ½Ð¸Ðµ Ñ Ð¿Ð¾Ð´ÐºÑ€ÐµÐ¿Ð»ÐµÐ½Ð¸ÐµÐ¼ (RL) ÑÐ²Ð»ÑÐµÑ‚ÑÑ Ð½Ð°Ð¿Ñ€Ð°Ð²Ð»ÐµÐ½Ð¸ÐµÐ¼ Ð¼Ð°ÑˆÐ¸Ð½Ð½Ð¾Ð³Ð¾ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ñ Ð¸ Ð¸Ð·ÑƒÑ‡Ð°ÐµÑ‚ Ð²Ð·Ð°Ð¸Ð¼Ð¾Ð´ÐµÐ¹ÑÑ‚Ð²Ð¸Ðµ Ð°Ð³ÐµÐ½Ñ‚Ð°, ÐºÐ¾Ñ‚Ð¾Ñ€Ð¾Ð¼Ñƒ Ð½ÐµÐ¾Ð±Ñ…Ð¾Ð´Ð¸Ð¼Ð¾ Ð¼Ð°ÐºÑÐ¸Ð¼Ð¸Ð·Ð¸Ñ€Ð¾Ð²Ð°Ñ‚ÑŒ Ð´Ð¾Ð»Ð³Ð¾Ð²Ñ€ÐµÐ¼ÐµÐ½Ð½Ñ‹Ð¹ Ð²Ñ‹Ð¸Ð³Ñ€Ñ‹Ñˆ Ð² Ð½ÐµÐºÐ¾Ñ‚Ð¾Ñ€Ð¾Ð¹ ÑÑ€ÐµÐ´Ðµ. ÐÐ³ÐµÐ½Ñ‚Ñƒ Ð½Ðµ ÑÐ¾Ð¾Ð±Ñ‰Ð°ÐµÑ‚ÑÑ ÑÐ²ÐµÐ´ÐµÐ½Ð¸Ð¹ Ð¾ Ð¿Ñ€Ð°Ð²Ð¸Ð»ÑŒÐ½Ð¾ÑÑ‚Ð¸ Ð´ÐµÐ¹ÑÑ‚Ð²Ð¸Ð¹, ÐºÐ°Ðº Ð² Ð±Ð¾Ð»ÑŒÑˆÐ¸Ð½ÑÑ‚Ð²Ðµ Ð·Ð°Ð´Ð°Ñ‡ Ð¼Ð°ÑˆÐ¸Ð½Ð½Ð¾Ð³Ð¾ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ñ, Ð²Ð¼ÐµÑÑ‚Ð¾ ÑÑ‚Ð¾Ð³Ð¾ Ð°Ð³ÐµÐ½Ñ‚ Ð´Ð¾Ð»Ð¶ÐµÐ½ Ð¾Ð¿Ñ€ÐµÐ´ÐµÐ»Ð¸Ñ‚ÑŒ Ð²Ñ‹Ð³Ð¾Ð´Ð½Ñ‹Ðµ Ð´ÐµÐ¹ÑÑ‚Ð²Ð¸Ñ ÑÐ°Ð¼Ð¾ÑÑ‚Ð¾ÑÑ‚ÐµÐ»ÑŒÐ½Ð¾ Ð¸ÑÐ¿Ñ€Ð¾Ð±Ð¾Ð²Ð°Ð² Ð¸Ñ…. Ð˜ÑÐ¿Ñ‹Ñ‚Ð°Ð½Ð¸Ðµ Ð´ÐµÐ¹ÑÑ‚Ð²Ð¸Ð¹ Ð¸ Ð¾Ñ‚ÑÑ€Ð¾Ñ‡ÐµÐ½Ð½Ð°Ñ Ð½Ð°Ð³Ñ€Ð°Ð´Ð° ÑÐ²Ð»ÑÑŽÑ‚ÑÑ Ð¾ÑÐ½Ð¾Ð²Ð½Ñ‹Ð¼Ð¸ Ð¾Ñ‚Ð»Ð¸Ñ‡Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ñ‹Ð¼Ð¸ Ð¿Ñ€Ð¸Ð·Ð½Ð°ÐºÐ°Ð¼Ð¸ RL.\n",
    "\n",
    "<img src=\"rl_simple.png\" style=\"width: 400px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"rl_intro.png\" style=\"width: 300px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ÐžÑÐ½Ð¾Ð²Ð½Ñ‹Ðµ ÑÐ¾ÑÑ‚Ð°Ð²Ð»ÑÑŽÑ‰Ð¸Ðµ Ð¼Ð¾Ð´ÐµÐ»Ð¸ RL:\n",
    "* $s_t$ - ÑÐ¾ÑÑ‚Ð¾ÑÐ½Ð¸Ðµ ÑÑ€ÐµÐ´Ñ‹ Ð² Ð¼Ð¾Ð¼ÐµÐ½Ñ‚ Ð²Ñ€ÐµÐ¼ÐµÐ½Ð¸ $t$,\n",
    "* $a_t$ - Ð´ÐµÐ¹ÑÑ‚Ð²Ð¸Ðµ, ÑÐ¾Ð²ÐµÑ€ÑˆÐ°ÐµÐ¼Ð¾Ðµ Ð°Ð³ÐµÐ½Ñ‚Ð¾Ð¼ Ð² Ð¼Ð¾Ð¼ÐµÐ½Ñ‚ Ð²Ñ€ÐµÐ¼ÐµÐ½Ð¸ $t$,\n",
    "* $r_t$ - Ð²Ð¾Ð·Ð½Ð°Ð³Ñ€Ð°Ð¶Ð´ÐµÐ½Ð¸Ðµ, Ð¿Ð¾Ð»ÑƒÑ‡Ð°ÐµÐ¼Ð¾Ðµ Ð°Ð³ÐµÐ½Ñ‚Ð¾Ð¼ Ð¿Ñ€Ð¸ ÑÐ¾Ð²ÐµÑ€ÑˆÐµÐ½Ð¸Ð¸ Ð´ÐµÐ¹ÑÑ‚Ð²Ð¸Ñ $a_t$,\n",
    "* $\\pi$ - ÑÑ‚Ñ€Ð°Ñ‚ÐµÐ³Ð¸Ñ Ð°Ð³ÐµÐ½Ñ‚Ð° - Ð¿Ð¾ÑÐ»ÐµÐ´Ð¾Ð²Ð°Ñ‚ÐµÐ»ÑŒÐ½Ð¾ÑÑ‚ÑŒ Ð´ÐµÐ¹ÑÑ‚Ð²Ð¸Ð¹ Ð¸Ð»Ð¸ Ð¿Ð»Ð°Ð½."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ð’ Ð¿Ñ€Ð¾ÑÑ‚ÐµÐ¹ÑˆÐ¸Ñ… Ð¼Ð¾Ð´ÐµÐ»ÑÑ… RL ÑÑ€ÐµÐ´Ð° Ð¿Ñ€ÐµÐ´ÑÑ‚Ð°Ð²Ð»ÑÐµÑ‚ÑÑ Ð² Ð²Ð¸Ð´Ðµ Ð¼Ð°Ñ€ÐºÐ¾Ð²ÑÐºÐ¾Ð³Ð¾ Ð¿Ñ€Ð¾Ñ†ÐµÑÑÐ°, Ð³Ð´Ðµ Ñ„ÑƒÐ½ÐºÑ†Ð¸Ñ Ð¿ÐµÑ€ÐµÑ…Ð¾Ð´Ð° Ð¾Ð¿Ñ€ÐµÐ´ÐµÐ»ÑÐµÑ‚ÑÑ ÐºÐ°Ðº $P(s' |s,a)$, Ñ‡Ñ‚Ð¾ Ð¾Ð·Ð½Ð°Ñ‡Ð°ÐµÑ‚ Ð²ÐµÑ€Ð¾ÑÑ‚Ð½Ð¾ÑÑ‚ÑŒ Ð¾ÐºÐ°Ð·Ð°Ñ‚ÑŒÑÑ Ð² ÑÐ¾ÑÑ‚Ð¾ÑÐ½Ð¸Ð¸ $s'$ Ð¿Ñ€Ð¸ ÑÐ¾Ð²ÐµÑ€ÑˆÐµÐ½Ð¸Ð¸ Ð´ÐµÐ¹ÑÑ‚Ð²Ð¸Ñ $a$ Ð² ÑÐ¾ÑÑ‚Ð¾ÑÐ½Ð¸Ð¸ $s$. Ð’Ð¾Ð·Ð½Ð°Ð³Ñ€Ð°Ð¶Ð´ÐµÐ½Ð¸Ðµ Ñ‚ÐµÐ¿ÐµÑ€ÑŒ Ð¾Ð¿Ñ€ÐµÐ´ÐµÐ»ÑÐµÑ‚ÑÑ ÐºÐ°Ðº $r(s,a,s')$.\n",
    "\n",
    "<img src=\"mdp.png\" style=\"width: 400px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ð‘ÑƒÐ´ÐµÐ¼ Ð¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÑŒÑÑ ÑÑ‚Ð°Ð½Ð´Ð°Ñ€Ñ‚Ð½Ñ‹Ð¼Ð¸ ÑÑ€ÐµÐ´Ð°Ð¼Ð¸, Ñ€ÐµÐ°Ð»Ð¸Ð·Ð¾Ð²Ð°Ð½Ð½Ñ‹Ð¼Ð¸ Ð² Ð±Ð¸Ð±Ð»Ð¸Ð¾Ñ‚ÐµÐºÐµ OpenAI Gym (https://gym.openai.com)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "env = gym.make(\"MountainCar-v0\")\n",
    "\n",
    "plt.imshow(env.render('rgb_array'))\n",
    "print(\"Observation space:\", env.observation_space)\n",
    "print(\"Action space:\", env.action_space)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gym interface\n",
    "\n",
    "ÐžÑÐ½Ð¾Ð²Ð½Ñ‹Ðµ Ð¼ÐµÑ‚Ð¾Ð´Ñ‹ ÐºÐ»Ð°ÑÑÐ° __Env__:\n",
    "* __reset()__ - Ð¸Ð½Ð¸Ñ†Ð¸Ð°Ð»Ð¸Ð·Ð°Ñ†Ð¸Ñ Ð¾ÐºÑ€ÑƒÐ¶ÐµÐ½Ð¸Ñ Ð² Ð½Ð°Ñ‡Ð°Ð»ÑŒÐ½Ð¾Ð¼ ÑÐ¾ÑÑ‚Ð¾ÑÐ½Ð¸Ð¸ _Ð²Ð¾Ð·Ð²Ñ€Ð°Ñ‰Ð°ÐµÑ‚ Ð¿ÐµÑ€Ð²Ð¾Ðµ Ð½Ð°Ð±Ð»ÑŽÐ´ÐµÐ½Ð¸Ðµ_,\n",
    "* __render()__ - Ð¿Ð¾ÐºÐ°Ð·Ð°Ñ‚ÑŒ Ñ‚ÐµÐºÑƒÑ‰ÐµÐµ ÑÐ¾ÑÑ‚Ð¾ÑÐ½Ð¸Ðµ ÑÑ€ÐµÐ´Ñ‹,\n",
    "* __step(a)__ - Ð²Ñ‹Ð¿Ð¾Ð»Ð½Ð¸Ñ‚ÑŒ Ð´ÐµÐ¹ÑÑ‚Ð²Ð¸Ðµ __a__ Ð¸ Ð¿Ð¾Ð»ÑƒÑ‡Ð¸Ñ‚ÑŒ (new observation, reward, is done, info)\n",
    " * _new observation_ - Ð½Ð¾Ð²Ð¾Ðµ Ð½Ð°Ð±Ð»ÑŽÐ´ÐµÐ½Ð¸Ðµ Ð¿Ð¾ÑÐ»Ðµ Ð²Ñ‹Ð¿Ð¾Ð»Ð½ÐµÐ½Ð¸Ñ Ð´ÐµÐ¹ÑÑ‚Ð²Ð¸Ñ __a__,\n",
    " * _reward_ - Ð²Ð¾Ð·Ð½Ð°Ð³Ñ€Ð¶Ð´ÐµÐ½Ð¸Ðµ (ÑÐºÐ°Ð»ÑÑ€) Ð·Ð° Ð²Ñ‹Ð¿Ð¾Ð»Ð½ÐµÐ½Ð½Ð¾Ðµ Ð´ÐµÐ¹ÑÑ‚Ð²Ð¸Ðµ __a__,\n",
    " * _is done_ - True, ÐµÑÐ»Ð¸ Ð¿Ñ€Ð¾Ñ†ÐµÑÑ (Ð¼Ð°Ñ€ÐºÐ¾Ð²ÑÐºÐ¸Ð¹!) Ð·Ð°Ð²ÐµÑ€ÑˆÐ¸Ð»ÑÑ, False Ð¸Ð½Ð°Ñ‡Ðµ,\n",
    " * _info_ - Ð´Ð¾Ð¿Ð¾Ð»Ð½Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ð°Ñ Ð¸Ð½Ñ„Ð¾Ñ€Ð¼Ð°Ñ†Ð¸Ñ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "obs0 = env.reset()\n",
    "print(\"initial observation code:\", obs0)\n",
    "print(\"taking action 2 (right)\")\n",
    "new_obs, reward, is_done, _ = env.step(2)\n",
    "\n",
    "print(\"new observation code:\", new_obs)\n",
    "print(\"reward:\", reward)\n",
    "print(\"is game over?:\", is_done)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ð—Ð°Ð´Ð°Ð½Ð¸Ðµ 1\n",
    "ÐÐ°ÑˆÐ° Ñ†ÐµÐ»ÑŒ, Ñ‡Ñ‚Ð¾Ð±Ñ‹ Ñ‚ÐµÐ»ÐµÐ¶ÐºÐ° Ð´Ð¾ÑÑ‚Ð¸Ð³Ð»Ð° Ñ„Ð»Ð°Ð³Ð°. ÐœÐ¾Ð´Ð¸Ñ„Ð¸Ñ†Ð¸Ñ€ÑƒÐ¹Ñ‚Ðµ ÐºÐ¾Ð´ Ð½Ð¸Ð¶Ðµ Ð´Ð»Ñ Ð²Ñ‹Ð¿Ð¾Ð»Ð½ÐµÐ½Ð¸Ñ ÑÑ‚Ð¾Ð³Ð¾ Ð·Ð°Ð´Ð°Ð½Ð¸Ñ:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TIME_LIMIT = 250\n",
    "env = gym.wrappers.TimeLimit(gym.envs.classic_control.MountainCarEnv(),\n",
    "                             max_episode_steps=TIME_LIMIT + 1)\n",
    "s = env.reset()\n",
    "actions = {'left': 0, 'stop': 1, 'right': 2}\n",
    "\n",
    "%matplotlib notebook\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "fig.show()\n",
    "\n",
    "for t in range(TIME_LIMIT):\n",
    "    ### Ð—Ð”Ð•Ð¡Ð¬ Ð²Ð°Ñˆ ÐºÐ¾Ð´ - Ð·Ð°ÑÑ‚Ð°Ð²ÑŒÑ‚Ðµ Ñ‚ÐµÐ»ÐµÐ¶ÐºÑƒ Ð´Ð¾Ð¹Ñ‚Ð¸ Ð´Ð¾ Ñ„Ð»Ð°Ð³Ð°! ###\n",
    "\n",
    "    #draw game image on display\n",
    "    ax.clear()\n",
    "    ax.imshow(env.render('rgb_array'))\n",
    "    fig.canvas.draw()\n",
    "    \n",
    "    if done:\n",
    "        print(\"Well done!\")\n",
    "        break\n",
    "else:    \n",
    "    print(\"Time limit exceeded. Try again.\")\n",
    "    \n",
    "env.close()\n",
    "assert s[0] > 0.47\n",
    "print(\"You solved it!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Ð’ÐµÑ€Ð¾ÑÑ‚Ð½Ð¾ÑÑ‚Ð½Ñ‹Ð¹ Ð¿Ð¾Ð´Ñ…Ð¾Ð´ Ðº RL\n",
    "\n",
    "ÐŸÑƒÑÑ‚ÑŒ Ð½Ð°ÑˆÐ° ÑÑ‚Ñ€Ð°Ñ‚ÐµÐ³Ð¸Ñ - ÑÑ‚Ð¾ Ð²ÐµÑ€Ð¾ÑÑ‚Ð½Ð¾ÑÑ‚Ð½Ð¾Ðµ Ñ€Ð°ÑÐ¿Ñ€ÐµÐ´ÐµÐ»ÐµÐ½Ð¸Ðµ:\n",
    "\n",
    "$\\pi(s,a) = P(a|s)$\n",
    "\n",
    "Ð Ð°ÑÑÐ¼Ð¾Ñ‚Ñ€Ð¸Ð¼ Ð¿Ñ€Ð¸Ð¼ÐµÑ€ Ñ Ð·Ð°Ð´Ð°Ñ‡ÐµÐ¹ Taxi [Dietterich, 2000]. Ð”Ð»Ñ Ð½ÐµÐµ Ð¼Ñ‹ Ð¼Ð¾Ð¶ÐµÐ¼ ÑÑ‡Ð¸Ñ‚Ð°Ñ‚ÑŒ, Ñ‡Ñ‚Ð¾ Ð½Ð°ÑˆÐ° ÑÑ‚Ñ€Ð°Ñ‚ÐµÐ³Ð¸Ñ - ÑÑ‚Ð¾ Ð´Ð²ÑƒÐ¼ÐµÑ€Ð½Ñ‹Ð¹ Ð¼Ð°ÑÑÐ¸Ð²."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "env = gym.make(\"Taxi-v2\")\n",
    "env.reset()\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_states = env.observation_space.n\n",
    "n_actions = env.action_space.n\n",
    "\n",
    "print(\"n_states=%i, n_actions=%i\"%(n_states,n_actions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ð—Ð°Ð´Ð°Ð½Ð¸Ðµ 2\n",
    "\n",
    "Ð¡Ð¾Ð·Ð´Ð°Ð´Ð¸Ð¼ \"Ñ€Ð°Ð²Ð½Ð¾Ð¼ÐµÑ€Ð½ÑƒÑŽ\" ÑÑ‚Ñ€Ð°Ñ‚ÐµÐ³Ð¸ÑŽ Ð² Ð²Ð¸Ð´Ðµ Ð´Ð²ÑƒÐ¼ÐµÑ€Ð½Ð¾Ð³Ð¾ Ð¼Ð°ÑÑÐ¸Ð²Ð° Ñ Ñ€Ð°Ð²Ð½Ð¾Ð¼ÐµÑ€Ð½Ñ‹Ð¼ Ñ€Ð°Ð¿ÑÑ€ÐµÐ´ÐµÐ»ÐµÐ½Ð¸ÐµÐ¼ Ð¿Ð¾ Ð´ÐµÐ¹ÑÑ‚Ð²Ð¸ÑÐ¼ Ð¸ ÑÐ³ÐµÐ½ÐµÑ€Ð¸Ñ€ÑƒÐµÐ¼ Ð¸Ð³Ñ€Ð¾Ð²ÑƒÑŽ ÑÐµÑÑÐ¸ÑŽ Ñ Ñ‚Ð°ÐºÐ¾Ð¹ ÑÑ‚Ñ€Ð°Ñ‚ÐµÐ³Ð¸ÐµÐ¹."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "policy = np.array([[1./n_actions for _ in range(n_actions)] for _ in range(n_states)])\n",
    "\n",
    "assert np.allclose(policy,1./n_actions)\n",
    "assert np.allclose(np.sum(policy,axis=1), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_session(policy,t_max=10**4):\n",
    "    states,actions = [],[]\n",
    "    total_reward = 0.\n",
    "    \n",
    "    s = env.reset()\n",
    "    \n",
    "    for t in range(t_max):\n",
    "        \n",
    "        ### Ð—Ð”Ð•Ð¡Ð¬ Ð²Ð°Ñˆ ÐºÐ¾Ð´ - Ð½ÑƒÐ¶Ð½Ð¾ Ð²Ñ‹Ð±Ñ€Ð°Ñ‚ÑŒ Ð´ÐµÐ¹ÑÑ‚Ð²Ð¸Ðµ Ð² ÑÐ¾Ð¾Ñ‚Ð²ÐµÑ‚ÑÐ²Ð¸Ð¸ ÑÐ¾ ÑÑ‚Ñ€Ð°Ñ‚ÐµÐ³Ð¸ÐµÐ¹ ###\n",
    "        a = None\n",
    "        \n",
    "        new_s,r,done,info = env.step(a)\n",
    "        \n",
    "        #Record state, action and add up reward to states,actions and total_reward accordingly. \n",
    "        states.append(s)\n",
    "        actions.append(a)\n",
    "        total_reward+=r\n",
    "        \n",
    "        s = new_s\n",
    "        if done:\n",
    "            break\n",
    "    return states,actions,total_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s,a,r = generate_session(policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ÐÐ°ÑˆÐ° Ð·Ð°Ð´Ð°Ñ‡Ð° - Ð²Ñ‹Ð´ÐµÐ»Ð¸Ñ‚ÑŒ Ð»ÑƒÑ‡ÑˆÐ¸Ðµ Ð´ÐµÐ¹ÑÑ‚Ð²Ð¸Ñ Ð¸ ÑÐ¾ÑÑ‚Ð¾ÑÐ½Ð¸Ñ, Ñ‚.Ðµ. Ñ‚Ð°ÐºÐ¸Ðµ, Ð¿Ñ€Ð¸ ÐºÐ¾Ñ‚Ð¾Ñ€Ñ‹Ñ… Ð±Ñ‹Ð»Ð¾ Ð»ÑƒÑ‡ÑˆÐµÐµ Ð²Ð¾Ð·Ð½Ð°Ð³Ñ€Ð°Ð¶Ð´ÐµÐ½Ð¸Ðµ:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def select_elites(states_batch,actions_batch,rewards_batch,percentile=50):\n",
    "    \"\"\"\n",
    "    Select states and actions from games that have rewards >= percentile\n",
    "    :param states_batch: list of lists of states, states_batch[session_i][t]\n",
    "    :param actions_batch: list of lists of actions, actions_batch[session_i][t]\n",
    "    :param rewards_batch: list of rewards, rewards_batch[session_i][t]\n",
    "    \n",
    "    :returns: elite_states,elite_actions, both 1D lists of states and respective actions from elite sessions\n",
    "    \"\"\"\n",
    "    \n",
    "    ### Ð—Ð”Ð•Ð¡Ð¬ Ð²Ð°Ñˆ ÐºÐ¾Ð´ - Ð½ÑƒÐ¶Ð½Ð¾ Ð½Ð°Ð¹Ñ‚Ð¸ Ð¿Ð¾Ñ€Ð¾Ð³ Ð²Ð¾Ð·Ð½Ð°Ð³Ñ€Ð°Ð¶Ð´ÐµÐ½Ð¸Ñ Ð¿Ð¾ Ð¸Ð·Ð²ÐµÑÑ‚Ð½Ð¾Ð¼Ñƒ Ð¿ÐµÑ€ÑÐµÐ½Ñ‚Ð¸Ð»ÑŽ ###\n",
    "    reward_threshold = None\n",
    "    \n",
    "    ### Ð—Ð”Ð•Ð¡Ð¬ Ð²Ð°Ñˆ ÐºÐ¾Ð´ - Ð² ÑÐ¾Ð¾Ñ‚Ð²ÐµÑ‚ÑÑ‚Ð²Ð¸Ð¸ Ñ Ð½Ð°Ð¹Ð´ÐµÐ½Ð½Ñ‹Ð¼ Ð¿Ð¾Ñ€Ð¾Ð³Ð¾Ð¼ - Ð¾Ñ‚Ð¾Ð±Ñ€Ð°Ñ‚ÑŒ Ð¿Ð¾Ð´Ñ…Ð¾Ð´ÑÑ‰Ð¸Ðµ ÑÐ¾ÑÑ‚Ð¾ÑÐ½Ð¸Ñ Ð¸ Ð´ÐµÐ¹ÑÑ‚Ð²Ð¸Ñ ###\n",
    "    elite_states  = None\n",
    "    elite_actions = None\n",
    "    \n",
    "    return elite_states,elite_actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "states_batch = [\n",
    "    [1,2,3],   #game1\n",
    "    [4,2,0,2], #game2\n",
    "    [3,1]      #game3\n",
    "]\n",
    "\n",
    "actions_batch = [\n",
    "    [0,2,4],   #game1\n",
    "    [3,2,0,1], #game2\n",
    "    [3,3]      #game3\n",
    "]\n",
    "rewards_batch = [\n",
    "    3,         #game1\n",
    "    4,         #game2\n",
    "    5,         #game3\n",
    "]\n",
    "\n",
    "test_result_0 = select_elites(states_batch,actions_batch,rewards_batch,percentile=0)\n",
    "test_result_40 = select_elites(states_batch,actions_batch,rewards_batch,percentile=30)\n",
    "test_result_90 = select_elites(states_batch,actions_batch,rewards_batch,percentile=90)\n",
    "test_result_100 = select_elites(states_batch,actions_batch,rewards_batch,percentile=100)\n",
    "\n",
    "assert np.all(test_result_0[0] == [1, 2, 3, 4, 2, 0, 2, 3, 1])  \\\n",
    "   and np.all(test_result_0[1] == [0, 2, 4, 3, 2, 0, 1, 3, 3]),\\\n",
    "        \"For percentile 0 you should return all states and actions in chronological order\"\n",
    "assert np.all(test_result_40[0] == [4, 2, 0, 2, 3, 1]) and \\\n",
    "        np.all(test_result_40[1] ==[3, 2, 0, 1, 3, 3]),\\\n",
    "        \"For percentile 30 you should only select states/actions from two first\"\n",
    "assert np.all(test_result_90[0] == [3,1]) and \\\n",
    "        np.all(test_result_90[1] == [3,3]),\\\n",
    "        \"For percentile 90 you should only select states/actions from one game\"\n",
    "assert np.all(test_result_100[0] == [3,1]) and\\\n",
    "       np.all(test_result_100[1] == [3,3]),\\\n",
    "        \"Please make sure you use >=, not >. Also double-check how you compute percentile.\"\n",
    "print(\"Ok!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ð¢ÐµÐ¿ÐµÑ€ÑŒ Ð¼Ñ‹ Ñ…Ð¾Ñ‚Ð¸Ð¼ Ð½Ð°Ð¿Ð¸ÑÐ°Ñ‚ÑŒ Ð¾Ð±Ð½Ð¾Ð²Ð»ÑÑŽÑ‰ÑƒÑŽÑÑ ÑÑ‚Ñ€Ð°Ñ‚ÐµÐ³Ð¸ÑŽ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def update_policy(elite_states,elite_actions):\n",
    "    \"\"\"\n",
    "    policy[s_i,a_i] ~ #[occurences of si and ai in elite states/actions]\n",
    "    \n",
    "    :param elite_states: 1D list of states from elite sessions\n",
    "    :param elite_actions: 1D list of actions from elite sessions\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    new_policy = np.zeros([n_states,n_actions])\n",
    "    \n",
    "    ### Ð—Ð”Ð•Ð¡Ð¬ Ð²Ð°Ñˆ ÐºÐ¾Ð´ - Ð¾Ð±Ð½Ð¾Ð²ÑÐ»ÐµÐ¼ ÑÑ‚Ñ€Ð°Ñ‚ÐµÐ³Ð¸ÑŽ - Ð½Ð¾Ñ€Ð¼Ð¸Ñ€ÑƒÐµÐ¼ Ð½Ð¾Ð²Ñ‹Ðµ Ñ‡Ð°ÑÑ‚Ð¾Ñ‚Ñ‹ Ð´ÐµÐ¹ÑÑ‚Ð²Ð¸Ð¹ Ð¸ Ð½Ðµ Ð·Ð°Ð±Ñ‹Ð²Ð°ÐµÐ¼ Ð¿Ñ€Ð¾ Ð½Ðµ Ð²ÑÑ‚Ñ€ÐµÑ‡Ð°ÑŽÑ‰Ð¸ÐµÑÑ ÑÐ¾ÑÑ‚Ð¾ÑÐ½Ð¸Ñ ###\n",
    "    \n",
    "    for state in range(n_states):\n",
    "        new_policy[state, a] = None\n",
    "       \n",
    "    \n",
    "    return new_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "elite_states, elite_actions = ([1, 2, 3, 4, 2, 0, 2, 3, 1], [0, 2, 4, 3, 2, 0, 1, 3, 3])\n",
    "\n",
    "\n",
    "new_policy = update_policy(elite_states,elite_actions)\n",
    "\n",
    "assert np.isfinite(new_policy).all(), \"Your new policy contains NaNs or +-inf. Make sure you don't divide by zero.\"\n",
    "assert np.all(new_policy>=0), \"Your new policy can't have negative action probabilities\"\n",
    "assert np.allclose(new_policy.sum(axis=-1),1), \"Your new policy should be a valid probability distribution over actions\"\n",
    "reference_answer = np.array([\n",
    "       [ 1.        ,  0.        ,  0.        ,  0.        ,  0.        ],\n",
    "       [ 0.5       ,  0.        ,  0.        ,  0.5       ,  0.        ],\n",
    "       [ 0.        ,  0.33333333,  0.66666667,  0.        ,  0.        ],\n",
    "       [ 0.        ,  0.        ,  0.        ,  0.5       ,  0.5       ]])\n",
    "assert np.allclose(new_policy[:4,:5],reference_answer)\n",
    "print(\"Ok!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ð’Ð¸Ð·ÑƒÐ°Ð»Ð¸Ð·Ð¸Ñ€Ð¸ÑƒÐµÐ¼ Ð½Ð°Ñˆ Ð¿Ñ€Ð¾Ñ†ÐµÑÑ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ñ Ð¸ Ð±ÑƒÐ´ÐµÐ¼ Ð¸Ð·Ð¼ÐµÑ€ÑÑ‚ÑŒ Ñ€Ð°ÑÐ¿Ñ€ÐµÐ´ÐµÐ»ÐµÐ½Ð¸Ðµ Ð¿Ð¾Ð»ÑƒÑ‡Ð°ÐµÐ¼Ñ‹Ñ… Ð·Ð° ÑÐµÑÑÐ¸ÑŽ Ð²Ð¾Ð·Ð½Ð°Ð³Ñ€Ð°Ð¶Ð´ÐµÐ½Ð¸Ð¹ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "def show_progress(rewards_batch,log, reward_range=[-990,+10]):\n",
    "    \"\"\"\n",
    "    A convenience function that displays training progress. \n",
    "    No cool math here, just charts.\n",
    "    \"\"\"\n",
    "    \n",
    "    mean_reward = np.mean(rewards_batch)\n",
    "    threshold = np.percentile(rewards_batch,percentile)\n",
    "    log.append([mean_reward,threshold])\n",
    "\n",
    "    clear_output(True)\n",
    "    print(\"mean reward = %.3f, threshold=%.3f\"%(mean_reward,threshold))\n",
    "    plt.figure(figsize=[8,4])\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.plot(list(zip(*log))[0],label='Mean rewards')\n",
    "    plt.plot(list(zip(*log))[1],label='Reward thresholds')\n",
    "    plt.legend(loc=4)\n",
    "    plt.grid()\n",
    "    \n",
    "    plt.subplot(1,2,2)\n",
    "    plt.hist(rewards_batch,range=reward_range);\n",
    "    plt.vlines([np.percentile(rewards_batch,percentile)],[0],[100],label=\"percentile\",color='red')\n",
    "    plt.legend(loc=1)\n",
    "    plt.grid()\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "policy = np.ones([n_states,n_actions])/n_actions \n",
    "n_sessions = 250  #sample this many sessions\n",
    "percentile = 50  #take this percent of session with highest rewards\n",
    "learning_rate = 0.5  #add this thing to all counts for stability\n",
    "\n",
    "log = []\n",
    "\n",
    "for i in range(100):\n",
    "    \n",
    "    ### Ð—Ð”Ð•Ð¡Ð¬ Ð²Ð°Ñˆ ÐºÐ¾Ð´ - Ð³ÐµÐ½ÐµÑ€Ð¸Ñ€ÑƒÐµÐ¼ n_sessions ÑÐµÑÑÐ¸Ð¹ ###\n",
    "    %time sessions = []\n",
    "    \n",
    "    states_batch,actions_batch,rewards_batch = zip(*sessions)\n",
    "\n",
    "    ### Ð—Ð”Ð•Ð¡Ð¬ Ð²Ð°Ñˆ ÐºÐ¾Ð´ - Ð¾Ñ‚Ð±Ð¸Ñ€Ð°ÐµÐ¼ Ð»ÑƒÑ‡ÑˆÐ¸Ðµ Ð´ÐµÐ¹ÑÑ‚Ð²Ð¸Ñ Ð¸ ÑÐ¾ÑÑ‚Ð¾ÑÐ½Ð¸Ñ ###\n",
    "    elite_states, elite_actions = None, None\n",
    "    \n",
    "    ### Ð—Ð”Ð•Ð¡Ð¬ Ð²Ð°Ñˆ ÐºÐ¾Ð´ - Ð¾Ð±Ð½Ð¾Ð²Ð»ÑÐµÐ¼ ÑÑ‚Ñ€Ð°Ñ‚ÐµÐ³Ð¸ÑŽ ###\n",
    "    new_policy = None\n",
    "    \n",
    "    policy = learning_rate*new_policy + (1-learning_rate)*policy\n",
    "    \n",
    "    #display results on chart\n",
    "    show_progress(rewards_batch,log)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Ð’Ð¸Ð´Ð¸Ð¼, Ñ‡Ñ‚Ð¾ Ð¼Ð¾Ð´ÐµÐ»ÑŒ Ð¾Ð±ÑƒÑ‡Ð°ÐµÑ‚ÑÑ. ÐžÑÐ¾Ð±ÐµÐ½Ð½Ð¾ÑÑ‚ÑŒ: Ð¿Ð¾ 50 Ð¿ÐµÑ€Ñ†ÐµÐ½Ñ‚Ð¸Ð»ÑŽ Ð¿Ð¾Ñ€Ð¾Ð³ ÑÑ‚Ñ€ÐµÐ¼Ð¸Ñ‚ÑÑ Ðº Ð¾Ð¿Ñ‚Ð¸Ð¼Ð°Ð»ÑŒÐ½Ð¾Ð¼Ñƒ Ð·Ð½Ð°Ñ‡ÐµÐ½Ð¸ÑŽ +20, Ñ…Ð¾Ñ‚Ñ Ð¸ Ð½Ðµ Ð¿Ñ€ÐµÐ²Ñ‹ÑˆÐ°ÐµÑ‚ +10, Ð° ÑÑ€ÐµÐ´Ð½ÐµÐµ Ð²Ð¾Ð·Ð½Ð°Ð³Ñ€Ð°Ð¶Ð´ÐµÐ½Ð¸Ðµ ÑÐ¸Ð»ÑŒÐ½Ð¾ ÐºÐ¾Ð»ÐµÐ±Ð»ÐµÑ‚ÑÑ Ð² Ñ€Ð°Ð¹Ð¾Ð½Ðµ (-80;-20). Ð­Ñ‚Ð¾ Ð¾Ð±ÑŠÑÑÐ½ÑÐµÑ‚ÑÑ Ð²ÐµÑ€Ð¾Ñ‚ÑÐ½Ð¾ÑÑ‚Ð¾Ð¹ Ð¿Ñ€Ð¸Ñ€Ð¾Ð´Ð¾Ð¹ ÑÑ‚Ñ€Ð°Ñ‚ÐµÐ³Ð¸Ð¸."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ÐŸÐ¾Ð´Ñ…Ð¾Ð´ Ð½Ð° Ð¾ÑÐ½Ð¾Ð²Ðµ Ñ„ÑƒÐ½ÐºÑ†Ð¸Ð¹ Ð¾Ñ†ÐµÐ½ÐºÐ¸"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ð Ð°ÑÑÐ¼Ð¾Ñ‚Ñ€Ð¸Ð¼ Ð°Ð»Ð³Ð¾Ñ€Ð¸Ñ‚Ð¼ Ð¸Ñ‚ÐµÑ€Ð°Ñ†Ð¸Ð¸ Ð¿Ð¾ Ð¾Ñ†ÐµÐ½ÐºÐ°Ð¼ ÑÐ¾ÑÑ‚Ð¾ÑÐ½Ð¸ÑÐ¼ $V$ (Value Iteration):\n",
    "\n",
    "---\n",
    "\n",
    "`1.` Initialize $V^{(0)}(s)=0$, for all $s$\n",
    "\n",
    "`2.` For $i=0, 1, 2, \\dots$\n",
    " \n",
    "`3.` $ \\quad V_{(i+1)}(s) = \\max_a \\sum_{s'} P(s' | s,a) \\cdot [ r(s,a,s') + \\gamma V_{i}(s')]$, for all $s$\n",
    "\n",
    "---\n",
    "\n",
    "ÐÐ° Ð¾ÑÐ½Ð¾Ð²Ðµ Ð¾Ñ†ÐµÐ½ÐºÐ¸ $V_i$ Ð¼Ð¾Ð¶Ð½Ð¾ Ð¿Ð¾ÑÑ‡Ð¸Ñ‚Ð°Ñ‚ÑŒ Ñ„ÑƒÐ½ÐºÑ†Ð¸ÑŽ Ð¾Ñ†ÐµÐ½ÐºÐ¸ $Q_i$ Ð´ÐµÐ¹ÑÑ‚Ð²Ð¸Ñ $a$ Ð² ÑÐ¾ÑÑ‚Ð¾ÑÐ½Ð¸Ð¸ $s$:\n",
    "\n",
    "$$Q_i(s, a) = \\sum_{s'} P(s' | s,a) \\cdot [ r(s,a,s') + \\gamma V_{i}(s')]$$\n",
    "\n",
    "$$V_{(i+1)}(s) = \\max_a Q_i(s,a)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ð—Ð°Ð´Ð°Ð´Ð¸Ð¼ Ð½Ð°Ð¿Ñ€ÑÐ¼ÑƒÑŽ Ð¼Ð¾Ð´ÐµÐ»ÑŒ MDP Ñ ÐºÐ°Ñ€Ñ‚Ð¸Ð½ÐºÐ¸ Ð² Ð½Ð°Ñ‡Ð°Ð»Ðµ Ñ‚ÐµÑ‚Ñ€Ð°Ð´ÐºÐ¸."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "transition_probs = {\n",
    "  's0':{\n",
    "    'a0': {'s0': 0.5, 's2': 0.5},\n",
    "    'a1': {'s2': 1}\n",
    "  },\n",
    "  's1':{\n",
    "    'a0': {'s0': 0.7, 's1': 0.1, 's2': 0.2},\n",
    "    'a1': {'s1': 0.95, 's2': 0.05}\n",
    "  },\n",
    "  's2':{\n",
    "    'a0': {'s0': 0.4, 's1': 0.6},\n",
    "    'a1': {'s0': 0.3, 's1': 0.3, 's2':0.4}\n",
    "  }\n",
    "}\n",
    "rewards = {\n",
    "  's1': {'a0': {'s0': +5}},\n",
    "  's2': {'a1': {'s0': -1}}\n",
    "}\n",
    "\n",
    "from mdp import MDP\n",
    "mdp = MDP(transition_probs, rewards, initial_state='s0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"mdp.get_all_states =\", mdp.get_all_states())\n",
    "print(\"mdp.get_possible_actions('s1') = \", mdp.get_possible_actions('s1'))\n",
    "print(\"mdp.get_next_states('s1', 'a0') = \", mdp.get_next_states('s1', 'a0'))\n",
    "print(\"mdp.get_reward('s1', 'a0', 's0') = \", mdp.get_reward('s1', 'a0', 's0'))\n",
    "print(\"mdp.get_transition_prob('s1', 'a0', 's0') = \", mdp.get_transition_prob('s1', 'a0', 's0'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ð—Ð°Ð´Ð°Ð½Ð¸Ðµ 3\n",
    "\n",
    "Ð ÐµÐ°Ð»Ð¸Ð·ÑƒÐµÐ¼ Ð¸Ñ‚ÐµÑ€Ð°Ñ†Ð¸Ð¾Ð½Ð½Ð¾Ðµ Ð²Ñ‹Ñ‡Ð¸ÑÐ»ÐµÐ½Ð¸Ðµ Ñ„ÑƒÐ½ÐºÑ†Ð¸Ð¹ $V$ Ð¸ $Q$ Ð¸ Ð¿Ñ€Ð¸Ð¼ÐµÐ½Ð¸Ð¼ Ð¸Ñ… Ð´Ð»Ñ Ð·Ð°Ð´Ð°Ð½Ð¾Ð³Ð¾ Ð²Ñ€ÑƒÑ‡Ð½ÑƒÑŽ MDP (ÑÐ¼. ÐºÐ°Ñ€Ñ‚Ð¸Ð½ÐºÑƒ Ð² Ð½Ð°Ñ‡Ð°Ð»Ðµ Ñ‚ÐµÑ‚Ñ€Ð°Ð´ÐºÐ¸) Ð¸ ÐºÐ»Ð°ÑÑÑ‡Ð¸Ñ‡ÐµÑÐºÐ¾Ð¹ Ð·Ð°Ð´Ð°Ñ‡Ð¸ FrozenLake."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_action_value(mdp, state_values, state, action, gamma):\n",
    "    \"\"\" Computes Q(s,a) as in formula above \"\"\"\n",
    "    \n",
    "    ### Ð—Ð”Ð•Ð¡Ð¬ Ð²Ð°Ñˆ ÐºÐ¾Ð´ - ÑÑ‡Ð¸Ñ‚Ð°ÐµÐ¼ Q Ð¿Ð¾ Ñ„Ð¾Ñ€Ð¼ÑƒÐ»Ðµ Ð²Ñ‹ÑˆÐµ ###\n",
    "    Q = None\n",
    "    \n",
    "    return Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_Vs = {s : i for i, s in enumerate(sorted(mdp.get_all_states()))}\n",
    "assert np.allclose(get_action_value(mdp, test_Vs, 's2', 'a1', 0.9), 0.69)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_new_state_value(mdp, state_values, state, gamma):\n",
    "    \"\"\" Computes next V(s) as in formula above. Please do not change state_values in process. \"\"\"\n",
    "    if mdp.is_terminal(state): return 0\n",
    "    \n",
    "    ### Ð—Ð”Ð•Ð¡Ð¬ Ð²Ð°Ñˆ ÐºÐ¾Ð´ - ÑÑ‡Ð¸Ñ‚Ð°ÐµÐ¼ V Ð¿Ð¾ Ñ„Ð¾Ñ€Ð¼ÑƒÐ»Ðµ Ð²Ñ‹ÑˆÐµ###\n",
    "    V = None\n",
    "    \n",
    "    return V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_Vs_copy = dict(test_Vs)\n",
    "assert np.allclose(get_new_state_value(mdp, test_Vs, 's0', 0.9), 1.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# parameters\n",
    "gamma = 0.9            # discount for MDP\n",
    "num_iter = 100         # maximum iterations, excluding initialization\n",
    "min_difference = 0.001 # stop VI if new values are this close to old values (or closer)\n",
    "\n",
    "# initialize V(s)\n",
    "state_values = {s : 0 for s in mdp.get_all_states()}\n",
    "\n",
    "\n",
    "for i in range(num_iter):\n",
    "    \n",
    "    # Compute new state values using the functions you defined above.\n",
    "    # It must be a dict {state : float V_new(state)}\n",
    "    \n",
    "    ### Ð—Ð”Ð•Ð¡Ð¬ Ð²Ð°Ñˆ ÐºÐ¾Ð´ - ÑÑ‡Ð¸Ñ‚Ð°ÐµÐ¼ V Ð´Ð»Ñ Ð²ÑÐµÑ… ÑÐ¾ÑÑ‚Ð¾ÑÐ½Ð¸Ð¹ ###\n",
    "    new_state_values = {}\n",
    "    \n",
    "    assert isinstance(new_state_values, dict)\n",
    "    \n",
    "    # Compute difference\n",
    "    diff = max(abs(new_state_values[s] - state_values[s]) for s in mdp.get_all_states())\n",
    "    print(\"iter %4i   |   diff: %6.5f   |   \"%(i, diff), end=\"\")\n",
    "    print('   '.join(\"V(%s) = %.3f\"%(s, v) for s,v in state_values.items()), end='\\n\\n')\n",
    "    state_values = new_state_values\n",
    "    \n",
    "    if diff < min_difference:\n",
    "        print(\"Terminated\"); break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"Final state values:\", state_values)\n",
    "\n",
    "assert abs(state_values['s0'] - 8.032)  < 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_optimal_action(mdp, state_values, state, gamma=0.9):\n",
    "    \"\"\" Finds optimal action using formula above. \"\"\"\n",
    "    if mdp.is_terminal(state): return None\n",
    "    \n",
    "    ### Ð—Ð”Ð•Ð¡Ð¬ Ð²Ð°Ñˆ ÐºÐ¾Ð´ - Ð²Ñ‹Ð±Ð¸Ñ€Ð°ÐµÐ¼ Ð¾Ð¿Ñ‚Ð¸Ð¼Ð°Ð»ÑŒÐ½Ð¾Ðµ Ð´ÐµÐ¹ÑÑ‚Ð²Ð¸Ðµ ###\n",
    "    \n",
    "    action = None\n",
    "    \n",
    "    \n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "assert get_optimal_action(mdp, state_values, 's0', gamma) == 'a1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from mdp import FrozenLakeEnv\n",
    "mdp = FrozenLakeEnv(slip_chance=0)\n",
    "\n",
    "mdp.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def value_iteration(mdp, state_values=None, gamma = 0.9, num_iter = 1000, min_difference = 1e-5):\n",
    "    \"\"\" performs num_iter value iteration steps starting from state_values. Same as before but in a function \"\"\"\n",
    "    state_values = state_values or {s : 0 for s in mdp.get_all_states()}\n",
    "    for i in range(num_iter):\n",
    "\n",
    "        # Compute new state values using the functions you defined above. It must be a dict {state : new_V(state)}\n",
    "        ### Ð—Ð”Ð•Ð¡Ð¬ Ð²Ð°Ñˆ ÐºÐ¾Ð´ - ÑÑ‡Ð¸Ñ‚Ð°ÐµÐ¼ V Ð´Ð»Ñ Ð²ÑÐµÑ… ÑÐ¾ÑÑ‚Ð¾ÑÐ½Ð¸Ð¹ ###\n",
    "        new_state_values = {}\n",
    "        \n",
    "        assert isinstance(new_state_values, dict)\n",
    "\n",
    "        # Compute difference\n",
    "        diff =  max(abs(new_state_values[s] - state_values[s]) for s in mdp.get_all_states())\n",
    "        \n",
    "        print(\"iter %4i   |   diff: %6.5f   |   V(start): %.3f \"%(i, diff, new_state_values[mdp._initial_state]))\n",
    "        \n",
    "        state_values = new_state_values\n",
    "        if diff < min_difference:\n",
    "            break\n",
    "            \n",
    "    return state_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "state_values = value_iteration(mdp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def draw_policy(mdp, state_values):\n",
    "    plt.figure(figsize=(3,3))\n",
    "    h,w = mdp.desc.shape\n",
    "    states = sorted(mdp.get_all_states())\n",
    "    V = np.array([state_values[s] for s in states])\n",
    "    Pi = {s: get_optimal_action(mdp, state_values, s, gamma) for s in states}\n",
    "    plt.imshow(V.reshape(w,h), cmap='gray', interpolation='none', clim=(0,1))\n",
    "    ax = plt.gca()\n",
    "    ax.set_xticks(np.arange(h)-.5)\n",
    "    ax.set_yticks(np.arange(w)-.5)\n",
    "    ax.set_xticklabels([])\n",
    "    ax.set_yticklabels([])\n",
    "    Y, X = np.mgrid[0:4, 0:4]\n",
    "    a2uv = {'left': (-1, 0), 'down':(0, -1), 'right':(1,0), 'up':(-1, 0)}\n",
    "    for y in range(h):\n",
    "        for x in range(w):\n",
    "            plt.text(x, y, str(mdp.desc[y,x].item()),\n",
    "                     color='g', size=12,  verticalalignment='center',\n",
    "                     horizontalalignment='center', fontweight='bold')\n",
    "            a = Pi[y, x]\n",
    "            if a is None: continue\n",
    "            u, v = a2uv[a]\n",
    "            plt.arrow(x, y,u*.3, -v*.3, color='m', head_width=0.1, head_length=0.1) \n",
    "    plt.grid(color='b', lw=2, ls='-')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from time import sleep\n",
    "\n",
    "mdp = FrozenLakeEnv(map_name='8x8',slip_chance=0.1)\n",
    "state_values = {s : 0 for s in mdp.get_all_states()}\n",
    "\n",
    "for i in range(30):\n",
    "    clear_output(True)\n",
    "    print(\"after iteration %i\"%i)\n",
    "    state_values = value_iteration(mdp, state_values, num_iter=1)\n",
    "    draw_policy(mdp, state_values)\n",
    "    sleep(0.5)\n",
    "# please ignore iter 0 at each step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Measure agent's average reward\n",
    "mdp = FrozenLakeEnv(slip_chance=0.25)\n",
    "state_values = value_iteration(mdp)\n",
    "\n",
    "total_rewards = []\n",
    "for game_i in range(1000):\n",
    "    s = mdp.reset()\n",
    "    rewards = []\n",
    "    for t in range(100):\n",
    "        s, r, done, _ = mdp.step(get_optimal_action(mdp, state_values, s, gamma))\n",
    "        rewards.append(r)\n",
    "        if done: break\n",
    "    total_rewards.append(np.sum(rewards))\n",
    "    \n",
    "print(\"average reward: \", np.mean(total_rewards))\n",
    "assert(0.6 <= np.mean(total_rewards) <= 0.7)\n",
    "print(\"Well done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Ð”Ð¾Ð¿ Ð»Ð¸Ñ‚ÐµÑ€Ð°Ñ‚ÑƒÑ€Ð°\n",
    "\n",
    "* ÐšÑƒÑ€Ñ Ð¾Ñ‚ YSDA https://github.com/yandexdataschool/Practical_RL\n",
    "* ÐšÐ½Ð¸Ð³Ð° Sutton, Barto Reinforcement learning: An Introduction\n",
    "* ÐšÑƒÑ€Ñ Ð¾Ñ‚ Udacity - https://classroom.udacity.com/courses/ud600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
